{
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat_minor": 5,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%additional_python_modules openpyxl\n",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nAdditional python modules to be included:\nopenpyxl\n",
					"output_type": "stream"
				}
			],
			"id": "4fd716f5-6e5c-4760-a6ac-f78cb34895b7"
		},
		{
			"cell_type": "code",
			"source": "import pandas as pd\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, lit\nimport boto3\nimport re\nfrom datetime import datetime\nimport os\nimport time",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nSession ID: fd3990e2-3942-4272-9770-acd90eb21522\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--additional-python-modules openpyxl\nWaiting for session fd3990e2-3942-4272-9770-acd90eb21522 to get into ready status...\nSession fd3990e2-3942-4272-9770-acd90eb21522 has been created.\n\n",
					"output_type": "stream"
				}
			],
			"id": "4c7ebf90"
		},
		{
			"cell_type": "code",
			"source": "# Configuração do  bucket\nbucket = \"pipeline-opea-bucket\"\narquivo_excel = \"s3://pipeline-opea-bucket/origem/dados_entrada.xlsx\"\ndata_hoje = datetime.now().strftime(\"%Y-%m-%d\")\n\n# Cria sessão Spark (já existe no Glue)\nspark = SparkSession.builder.appName(\"Pipeline OPEA\").getOrCreate()\n\nprint(\"Bucket:\", bucket)\nprint(\"Data:\", data_hoje)\nprint(\"Spark:\", spark.version)",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Bucket: pipeline-opea-bucket\nData: 2025-12-01\nSpark: 3.3.0-amzn-1\n",
					"output_type": "stream"
				}
			],
			"id": "e740e777"
		},
		{
			"cell_type": "markdown",
			"source": "## Parte 1: Camada Raw",
			"metadata": {},
			"id": "4bc0f093"
		},
		{
			"cell_type": "markdown",
			"source": "### Imports",
			"metadata": {
				"tags": []
			},
			"id": "35fbd707"
		},
		{
			"cell_type": "markdown",
			"source": "# Pipeline OPEA - Excel para S3\n\nPipeline de dados com arquitetura medallion (Raw → Stage → Analytics)",
			"metadata": {},
			"id": "2e032aff"
		},
		{
			"cell_type": "markdown",
			"source": "### Validações",
			"metadata": {},
			"id": "3eccfe8b"
		},
		{
			"cell_type": "code",
			"source": "# Lista para guardar erros\nerros = []\n\ndef adicionar_erro(linha, campo, valor, motivo):\n    erros.append({\n        \"linha\": linha,\n        \"campo\": campo,\n        \"valor\": valor,\n        \"motivo\": motivo\n    })\n\nprint(\"OK\")",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "OK\n",
					"output_type": "stream"
				}
			],
			"id": "0b3d039a"
		},
		{
			"cell_type": "code",
			"source": "# Valida CPF\ndef validar_cpf(cpf):\n    if pd.isna(cpf):\n        return False\n    if re.match(r'^\\d{3}\\.\\d{3}\\.\\d{3}-\\d{2}$', str(cpf)):\n        return True\n    return False\n\n# Valida CEP\ndef validar_cep(cep):\n    if pd.isna(cep):\n        return False\n    if re.match(r'^\\d{5}-\\d{3}$', str(cep)):\n        return True\n    return False\n\n# Valida Email\ndef validar_email(email):\n    if pd.isna(email):\n        return False\n    if '@' in str(email) and '.' in str(email):\n        return True\n    return False\n\n# Valida Data\ndef validar_data(data):\n    if pd.isna(data):\n        return False\n    try:\n        datetime.strptime(str(data), \"%Y-%m-%d\")\n        return True\n    except:\n        return False\n\n# Valida Status\ndef validar_status(status):\n    if pd.isna(status):\n        return False\n    if str(status).lower() in [\"ativo\", \"inativo\", \"suspenso\"]:\n        return True\n    return False\n\nprint(\"Funções criadas\")",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "Funções criadas\n",
					"output_type": "stream"
				}
			],
			"id": "dfcff5e3"
		},
		{
			"cell_type": "markdown",
			"source": "### Leitura dos dados",
			"metadata": {},
			"id": "3d7de9b2"
		},
		{
			"cell_type": "code",
			"source": "# Lê as abas do Excel com pandas (pq Spark não lê Excel direto)\nclientes_pd = pd.read_excel(arquivo_excel, sheet_name=\"clientes\")\nenderecos_pd = pd.read_excel(arquivo_excel, sheet_name=\"enderecos\")\n\n# Converte tudo para string antes de criar Spark DataFrame\nclientes_pd = clientes_pd.astype(str)\nenderecos_pd = enderecos_pd.astype(str)\n\n# Converte para Spark DataFrame\nclientes = spark.createDataFrame(clientes_pd)\nenderecos = spark.createDataFrame(enderecos_pd)\n\nprint(\"Clientes:\", clientes.count())\nprint(\"Endereços:\", enderecos.count())\n\n# Mostra os dados\nclientes.show(5)\nenderecos.show(5)",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Clientes: 16\nEndereços: 15\n+----------+--------------------+--------------------+--------------+---------------+--------+-------------------+\n|id_cliente|                nome|               email|           cpf|data_nascimento|  status|        data_evento|\n+----------+--------------------+--------------------+--------------+---------------+--------+-------------------+\n|         1|          João Silva|joao.silva@email.com|123.456.789-10|     1990-05-15|   ativo|2024-01-10 08:30:00|\n|         1|João da Silva Santos|joao.silva@email.com|123.456.789-10|     1990-05-15|   ativo|2024-01-15 10:45:00|\n|         2|        Maria Santos|maria.santos@emai...|234.567.890-21|     1985-03-22| inativo|2024-01-08 14:20:00|\n|         3|      Pedro Oliveira|pedro.oliveira@em...|345.678.901-32|     1992-07-30|suspenso|2024-01-12 09:15:00|\n|         4|           Ana Costa| ana.costa@email.com|456.789.012-43|     1988-11-18|   ativo|2024-01-14 16:30:00|\n+----------+--------------------+--------------------+--------------+---------------+--------+-------------------+\nonly showing top 5 rows\n\n+-----------+----------+---------+--------------------+------+-----------------+---------------+--------------+------+-------------------+\n|id_endereco|id_cliente|      cep|          logradouro|numero|      complemento|         bairro|        cidade|estado|        data_evento|\n+-----------+----------+---------+--------------------+------+-----------------+---------------+--------------+------+-------------------+\n|       1001|         1|01310-100|    Avenida Paulista|1000.0|         Apto 501|     Bela Vista|     São Paulo|    SP|2024-01-10 08:35:00|\n|       1002|         1|01331-000|    Avenida Paulista|2000.0|        Apto 1501|Cerqueira César|     São Paulo|    SP|2024-01-15 10:50:00|\n|       2001|         2|30140-071|        Rua da Bahia|1500.0|              nan|         Centro|Belo Horizonte|    MG|2024-01-08 14:25:00|\n|       4001|         4|22250-040|   Avenida Atlântica|3000.0|Bloco A, Apto 302|     Copacabana|Rio de Janeiro|    RJ|2024-01-14 16:35:00|\n|       5001|         5|70040-020|Esplanada dos Min...| 100.0|         Sala 500|         Centro|      Brasília|    DF|2024-01-09 11:05:00|\n+-----------+----------+---------+--------------------+------+-----------------+---------------+--------------+------+-------------------+\nonly showing top 5 rows\n\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
					"output_type": "stream"
				}
			],
			"id": "d6b81f18"
		},
		{
			"cell_type": "markdown",
			"source": "### Processamento de clientes",
			"metadata": {},
			"id": "678ba465"
		},
		{
			"cell_type": "code",
			"source": "# Valida clientes usando Spark\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import BooleanType\n\n# Funções UDF para validação\nvalida_cpf_udf = udf(validar_cpf, BooleanType())\nvalida_email_udf = udf(validar_email, BooleanType())\nvalida_data_udf = udf(validar_data, BooleanType())\nvalida_status_udf = udf(validar_status, BooleanType())\n\n# Aplica validações\nclientes_com_validacao = clientes \\\n    .withColumn(\"cpf_valido\", valida_cpf_udf(col(\"cpf\"))) \\\n    .withColumn(\"email_valido\", valida_email_udf(col(\"email\"))) \\\n    .withColumn(\"data_nasc_valido\", valida_data_udf(col(\"data_nascimento\"))) \\\n    .withColumn(\"data_evento_valido\", valida_data_udf(col(\"data_evento\"))) \\\n    .withColumn(\"status_valido\", valida_status_udf(col(\"status\")))\n\n# Filtra só os válidos\nclientes_ok = clientes_com_validacao.filter(\n    (col(\"cpf_valido\") == True) & \n    (col(\"email_valido\") == True) & \n    (col(\"data_nasc_valido\") == True) & \n    (col(\"data_evento_valido\") == True) & \n    (col(\"status_valido\") == True) &\n    (col(\"id_cliente\").isNotNull()) &\n    (col(\"nome\").isNotNull())\n)\n\n# Remove colunas de validação\nclientes_ok = clientes_ok.drop(\"cpf_valido\", \"email_valido\", \"data_nasc_valido\", \"data_evento_valido\", \"status_valido\")\n\n# Pega os inválidos para o log\nclientes_invalidos = clientes_com_validacao.filter(\n    (col(\"cpf_valido\") == False) | \n    (col(\"email_valido\") == False) | \n    (col(\"data_nasc_valido\") == False) | \n    (col(\"data_evento_valido\") == False) | \n    (col(\"status_valido\") == False) |\n    (col(\"id_cliente\").isNull()) |\n    (col(\"nome\").isNull())\n)\n\nprint(\"Clientes OK:\", clientes_ok.count())\nprint(\"Clientes com erro:\", clientes_invalidos.count())",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "Clientes OK: 1\nClientes com erro: 15\n",
					"output_type": "stream"
				}
			],
			"id": "4427f639"
		},
		{
			"cell_type": "markdown",
			"source": "### Processamento de endereços",
			"metadata": {},
			"id": "44ef8399"
		},
		{
			"cell_type": "code",
			"source": "# Valida endereços\nvalida_cep_udf = udf(validar_cep, BooleanType())\n\n# Pega lista de IDs de clientes válidos\nids_clientes_ok = [row.id_cliente for row in clientes_ok.select(\"id_cliente\").collect()]\n\n# Aplica validações\nenderecos_com_validacao = enderecos \\\n    .withColumn(\"cep_valido\", valida_cep_udf(col(\"cep\"))) \\\n    .withColumn(\"data_evento_valido\", valida_data_udf(col(\"data_evento\"))) \\\n    .withColumn(\"cliente_existe\", col(\"id_cliente\").isin(ids_clientes_ok))\n\n# Filtra só os válidos\nenderecos_ok = enderecos_com_validacao.filter(\n    (col(\"cep_valido\") == True) & \n    (col(\"data_evento_valido\") == True) & \n    (col(\"cliente_existe\") == True) &\n    (col(\"id_endereco\").isNotNull())\n)\n\n# Remove colunas de validação\nenderecos_ok = enderecos_ok.drop(\"cep_valido\", \"data_evento_valido\", \"cliente_existe\")\n\n# Pega os inválidos\nenderecos_invalidos = enderecos_com_validacao.filter(\n    (col(\"cep_valido\") == False) | \n    (col(\"data_evento_valido\") == False) | \n    (col(\"cliente_existe\") == False) |\n    (col(\"id_endereco\").isNull())\n)\n\nprint(\"Endereços OK:\", enderecos_ok.count())\nprint(\"Endereços com erro:\", enderecos_invalidos.count())",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "Endereços OK: 0\nEndereços com erro: 15\n",
					"output_type": "stream"
				}
			],
			"id": "016fd94d"
		},
		{
			"cell_type": "markdown",
			"source": "### Salvar erros",
			"metadata": {},
			"id": "d427bcad"
		},
		{
			"cell_type": "code",
			"source": "# Salva os erros em CSV\nif clientes_invalidos.count() > 0 or enderecos_invalidos.count() > 0:\n    # Junta os erros\n    erros_cli = clientes_invalidos.toPandas()\n    erros_end = enderecos_invalidos.toPandas()\n    \n    # Salva como CSV\n    if len(erros_cli) > 0:\n        erros_cli.to_csv(f\"erros_clientes_{data_hoje}.csv\", index=False)\n        print(f\"Erros de clientes: {len(erros_cli)}\")\n    \n    if len(erros_end) > 0:\n        erros_end.to_csv(f\"erros_enderecos_{data_hoje}.csv\", index=False)\n        print(f\"Erros de endereços: {len(erros_end)}\")\nelse:\n    print(\"Sem erros!\")",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "Erros de clientes: 15\nErros de endereços: 15\n",
					"output_type": "stream"
				}
			],
			"id": "eed4c3ad"
		},
		{
			"cell_type": "markdown",
			"source": "### Salvar na camada Raw",
			"metadata": {},
			"id": "87a8497b"
		},
		{
			"cell_type": "code",
			"source": "# Adiciona data de processamento em todosos dados brutos\nclientes = clientes.withColumn(\"data_processamento\", lit(data_hoje))\nenderecos = enderecos.withColumn(\"data_processamento\", lit(data_hoje))\n\nprint(\"OK\")",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "OK\n",
					"output_type": "stream"
				}
			],
			"id": "59fec28f"
		},
		{
			"cell_type": "markdown",
			"source": "### Função de salvar",
			"metadata": {},
			"id": "e47a4f43"
		},
		{
			"cell_type": "code",
			"source": "def salvar_s3(df, nome):\n    caminho_s3 = f\"s3://{bucket}/raw/{nome}/data_processamento={data_hoje}/\"\n    \n    try:\n        df.write.mode(\"overwrite\").parquet(caminho_s3, compression=\"snappy\")\n        print(f\"Salvo: {caminho_s3}\")\n        return True\n    except Exception as e:\n        print(f\"Erro: {e}\")\n        return False\n\nprint(\"OK\")",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "OK\n",
					"output_type": "stream"
				}
			],
			"id": "34457044"
		},
		{
			"cell_type": "code",
			"source": "# Salva TODOS os dados brutos (sem filtro)\nsalvar_s3(clientes, \"clientes\")\nsalvar_s3(enderecos, \"enderecos\")",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "Salvo: s3://pipeline-opea-bucket/raw/clientes/data_processamento=2025-12-01/\nSalvo: s3://pipeline-opea-bucket/raw/enderecos/data_processamento=2025-12-01/\nTrue\n",
					"output_type": "stream"
				}
			],
			"id": "d3931ecd"
		},
		{
			"cell_type": "markdown",
			"source": "## Parte 2: Camada Stage",
			"metadata": {},
			"id": "10d3bcdd"
		},
		{
			"cell_type": "code",
			"source": "caminho_raw_clientes = f\"s3://{bucket}/raw/clientes/\"\ncaminho_raw_enderecos = f\"s3://{bucket}/raw/enderecos/\"\n\ndf_raw_clientes = spark.read.parquet(caminho_raw_clientes)\ndf_raw_enderecos = spark.read.parquet(caminho_raw_enderecos)\n\nprint(\"Clientes Raw:\", df_raw_clientes.count())\nprint(\"Endereços Raw:\", df_raw_enderecos.count())\n\n# aplica as validações na Stage\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import BooleanType\n\nvalida_cpf_udf = udf(validar_cpf, BooleanType())\nvalida_email_udf = udf(validar_email, BooleanType())\nvalida_data_udf = udf(validar_data, BooleanType())\nvalida_status_udf = udf(validar_status, BooleanType())\nvalida_cep_udf = udf(validar_cep, BooleanType())\n\n# Valida clientes\nclientes_validados = df_raw_clientes \\\n    .withColumn(\"cpf_valido\", valida_cpf_udf(col(\"cpf\"))) \\\n    .withColumn(\"email_valido\", valida_email_udf(col(\"email\"))) \\\n    .withColumn(\"data_nasc_valido\", valida_data_udf(col(\"data_nascimento\"))) \\\n    .withColumn(\"data_evento_valido\", valida_data_udf(col(\"data_evento\"))) \\\n    .withColumn(\"status_valido\", valida_status_udf(col(\"status\")))\n\n# Filtra apenas válidos\ndf_raw_clientes = clientes_validados.filter(\n    (col(\"cpf_valido\") == True) & \n    (col(\"email_valido\") == True) & \n    (col(\"data_nasc_valido\") == True) & \n    (col(\"data_evento_valido\") == True) & \n    (col(\"status_valido\") == True) &\n    (col(\"id_cliente\").isNotNull()) &\n    (col(\"nome\").isNotNull())\n).drop(\"cpf_valido\", \"email_valido\", \"data_nasc_valido\", \"data_evento_valido\", \"status_valido\")\n\n# Valida endereços\nids_clientes_validos = [row.id_cliente for row in df_raw_clientes.select(\"id_cliente\").collect()]\n\nenderecos_validados = df_raw_enderecos \\\n    .withColumn(\"cep_valido\", valida_cep_udf(col(\"cep\"))) \\\n    .withColumn(\"data_evento_valido\", valida_data_udf(col(\"data_evento\"))) \\\n    .withColumn(\"cliente_existe\", col(\"id_cliente\").isin(ids_clientes_validos))\n\ndf_raw_enderecos = enderecos_validados.filter(\n    (col(\"cep_valido\") == True) & \n    (col(\"data_evento_valido\") == True) & \n    (col(\"cliente_existe\") == True) &\n    (col(\"id_endereco\").isNotNull())\n).drop(\"cep_valido\", \"data_evento_valido\", \"cliente_existe\")\n\nprint(\"Clientes válidos:\", df_raw_clientes.count())\nprint(\"Endereços válidos:\", df_raw_enderecos.count())",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "Clientes Raw: 16\nEndereços Raw: 15\nClientes válidos: 1\nEndereços válidos: 0\n",
					"output_type": "stream"
				}
			],
			"id": "a11cf454-5f8c-4e3b-884d-e1df813938a8"
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number, current_timestamp\n\njanela_clientes = Window.partitionBy(\"id_cliente\").orderBy(col(\"data_evento\").desc())\n\nclientes_stage = df_raw_clientes \\\n    .withColumn(\"rn\", row_number().over(janela_clientes)) \\\n    .filter(col(\"rn\") == 1) \\\n    .drop(\"rn\") \\\n    .withColumn(\"data_atualizacao\", current_timestamp())\n\nprint(\"Clientes Stage:\", clientes_stage.count())",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 13,
			"outputs": [
				{
					"name": "stdout",
					"text": "Clientes Stage: 1\n",
					"output_type": "stream"
				}
			],
			"id": "bb82cbe3-5978-425b-af4c-bb78aa7ad6e9"
		},
		{
			"cell_type": "code",
			"source": "janela_enderecos = Window.partitionBy(\"id_endereco\").orderBy(col(\"data_evento\").desc())\n\nenderecos_stage = df_raw_enderecos \\\n    .withColumn(\"rn\", row_number().over(janela_enderecos)) \\\n    .filter(col(\"rn\") == 1) \\\n    .drop(\"rn\") \\\n    .withColumn(\"data_atualizacao\", current_timestamp())\n\nprint(\"Endereços Stage:\", enderecos_stage.count())",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 14,
			"outputs": [
				{
					"name": "stdout",
					"text": "Endereços Stage: 0\n",
					"output_type": "stream"
				}
			],
			"id": "bbbfe10e-20b6-4cbe-a81a-fb595c9aabe8"
		},
		{
			"cell_type": "code",
			"source": "caminho_stage_clientes = f\"s3://{bucket}/stage/clientes/\"\ncaminho_stage_enderecos = f\"s3://{bucket}/stage/enderecos/\"\n\nclientes_stage.write.mode(\"overwrite\").parquet(caminho_stage_clientes, compression=\"snappy\")\nenderecos_stage.write.mode(\"overwrite\").parquet(caminho_stage_enderecos, compression=\"snappy\")\n\nprint(\"Dados salvos na camada Stage!\")",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 15,
			"outputs": [
				{
					"name": "stdout",
					"text": "Dados salvos na camada Stage!\n",
					"output_type": "stream"
				}
			],
			"id": "e57a0812-202d-436e-bddf-c1060fb03a5e"
		},
		{
			"cell_type": "markdown",
			"source": "## Parte 3: Camada Analytics",
			"metadata": {},
			"id": "fec98880"
		},
		{
			"cell_type": "code",
			"source": "# Lê os dados da camada Stage\ndf_stage_clientes = spark.read.parquet(caminho_stage_clientes)\ndf_stage_enderecos = spark.read.parquet(caminho_stage_enderecos)\n\nprint(\"Clientes Stage:\", df_stage_clientes.count())\nprint(\"Endereços Stage:\", df_stage_enderecos.count())",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 16,
			"outputs": [
				{
					"name": "stdout",
					"text": "Clientes Stage: 1\nEndereços Stage: 0\n",
					"output_type": "stream"
				}
			],
			"id": "79ae7d55-65cb-4486-a90c-e4a1b9a7ed2f"
		},
		{
			"cell_type": "code",
			"source": "# Filtra só clientes ativos\nclientes_ativos = df_stage_clientes.filter(col(\"status\") == \"ativo\")\n\nprint(\"Clientes ativos:\", clientes_ativos.count())\nclientes_ativos.show(5)",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 17,
			"outputs": [
				{
					"name": "stdout",
					"text": "Clientes ativos: 1\n+----------+--------------------+--------------------+--------------+---------------+------+-----------+------------------+--------------------+\n|id_cliente|                nome|               email|           cpf|data_nascimento|status|data_evento|data_processamento|    data_atualizacao|\n+----------+--------------------+--------------------+--------------+---------------+------+-----------+------------------+--------------------+\n|       104|Cliente Data Even...|data.evento.inval...|123.456.789-14|     1990-01-01| ativo| 2024-01-10|        2025-12-01|2025-12-01 18:04:...|\n+----------+--------------------+--------------------+--------------+---------------+------+-----------+------------------+--------------------+\n",
					"output_type": "stream"
				}
			],
			"id": "c6596db7-2bca-42c5-89a7-5437ac95bb79"
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql.functions import to_date, datediff, floor\n\nclientes_com_idade = clientes_ativos.withColumn(\n    \"idade\",\n    floor(datediff(lit(data_hoje), to_date(col(\"data_nascimento\"), \"yyyy-MM-dd\")) / 365.25)\n)\n\nprint(\"Idade calculada!\")\nclientes_com_idade.select(\"nome\", \"data_nascimento\", \"idade\").show(5)",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 18,
			"outputs": [
				{
					"name": "stdout",
					"text": "Idade calculada!\n+--------------------+---------------+-----+\n|                nome|data_nascimento|idade|\n+--------------------+---------------+-----+\n|Cliente Data Even...|     1990-01-01|   35|\n+--------------------+---------------+-----+\n",
					"output_type": "stream"
				}
			],
			"id": "efafda92-a238-4207-ba0f-92bffd92a7f8"
		},
		{
			"cell_type": "code",
			"source": "enderecos_renamed = df_stage_enderecos \\\n    .withColumnRenamed(\"data_evento\", \"endereco_data_evento\") \\\n    .withColumnRenamed(\"data_processamento\", \"endereco_data_processamento\") \\\n    .withColumnRenamed(\"data_atualizacao\", \"endereco_data_atualizacao\")\n\nanalytics = clientes_com_idade.join(enderecos_renamed, on=\"id_cliente\", how=\"left\")\n\nprint(\"Total de registros:\", analytics.count())\nanalytics.show(5)",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 19,
			"outputs": [
				{
					"name": "stdout",
					"text": "Total de registros: 1\n+----------+--------------------+--------------------+--------------+---------------+------+-----------+------------------+--------------------+-----+-----------+----+----------+------+-----------+------+------+------+--------------------+---------------------------+-------------------------+\n|id_cliente|                nome|               email|           cpf|data_nascimento|status|data_evento|data_processamento|    data_atualizacao|idade|id_endereco| cep|logradouro|numero|complemento|bairro|cidade|estado|endereco_data_evento|endereco_data_processamento|endereco_data_atualizacao|\n+----------+--------------------+--------------------+--------------+---------------+------+-----------+------------------+--------------------+-----+-----------+----+----------+------+-----------+------+------+------+--------------------+---------------------------+-------------------------+\n|       104|Cliente Data Even...|data.evento.inval...|123.456.789-14|     1990-01-01| ativo| 2024-01-10|        2025-12-01|2025-12-01 18:04:...|   35|       null|null|      null|  null|       null|  null|  null|  null|                null|                       null|                     null|\n+----------+--------------------+--------------------+--------------+---------------+------+-----------+------------------+--------------------+-----+-----------+----+----------+------+-----------+------+------+------+--------------------+---------------------------+-------------------------+\n",
					"output_type": "stream"
				}
			],
			"id": "27d794bb-6bf3-4c94-b6e9-4ad3c360d78e"
		},
		{
			"cell_type": "code",
			"source": "caminho_analytics = f\"s3://{bucket}/analytics/clientes/\"\n\nanalytics.orderBy(\"id_cliente\").coalesce(1) \\\n    .write.mode(\"overwrite\").parquet(caminho_analytics, compression=\"snappy\")\n\nprint(f\"Dados salvos: {caminho_analytics}\")",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 20,
			"outputs": [
				{
					"name": "stdout",
					"text": "Dados salvos: s3://pipeline-opea-bucket/analytics/clientes/\n",
					"output_type": "stream"
				}
			],
			"id": "49dfb23b-b8b2-4f86-ad5f-f11a0beb42e2"
		},
		{
			"cell_type": "markdown",
			"source": "## Parte 4: Catálogo e Athena",
			"metadata": {},
			"id": "7194535d"
		},
		{
			"cell_type": "code",
			"source": "glue_client = boto3.client('glue', region_name='us-east-1')\ndatabase_name = 'clientes'\n\ntry:\n    glue_client.create_database(\n        DatabaseInput={\n            'Name': database_name,\n            'Description': 'Banco de dados para analytics de clientes'\n        }\n    )\n    print(f\"Banco '{database_name}' criado!\")\nexcept glue_client.exceptions.AlreadyExistsException:\n    print(f\"Banco '{database_name}' já existe\")\n\nrole_arn = \"arn:aws:iam::816648956948:role/service-role/AWSGlueServiceRole-brendon\"\nprint(f\"Role ARN: {role_arn}\")",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "Banco 'clientes' já existe\nRole ARN: arn:aws:iam::816648956948:role/service-role/AWSGlueServiceRole-brendon\n",
					"output_type": "stream"
				}
			],
			"id": "a99d8f36-3702-4b9c-a941-5fb7389c1577"
		},
		{
			"cell_type": "code",
			"source": "crawler_name = 'crawler-analytics-clientes'\ns3_path = f\"s3://{bucket}/analytics/clientes/\"\n\ntry:\n    glue_client.create_crawler(\n        Name=crawler_name,\n        Role=role_arn,\n        DatabaseName=database_name,\n        Targets={'S3Targets': [{'Path': s3_path}]},\n        Schedule='cron(0 */6 * * ? *)',\n        SchemaChangePolicy={'UpdateBehavior': 'LOG', 'DeleteBehavior': 'LOG'},\n        RecrawlPolicy={'RecrawlBehavior': 'CRAWL_NEW_FOLDERS_ONLY'}\n    )\n    print(f\"Crawler '{crawler_name}' criado!\")\nexcept glue_client.exceptions.AlreadyExistsException:\n    print(f\"Crawler '{crawler_name}' já existe\")",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "Crawler 'crawler-analytics-clientes' já existe\n",
					"output_type": "stream"
				}
			],
			"id": "9637b877-618f-4107-ab5f-f9bd8216f4cb"
		},
		{
			"cell_type": "code",
			"source": "try:\n    glue_client.start_crawler(Name=crawler_name)\n    print(f\"Crawler '{crawler_name}' iniciado!\")\nexcept glue_client.exceptions.CrawlerRunningException:\n    print(\"Crawler já está executando\")",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "Crawler já está executando\n",
					"output_type": "stream"
				}
			],
			"id": "34420796-2a53-46df-ab9c-e7a20bb67e89"
		},
		{
			"cell_type": "code",
			"source": "for i in range(10):\n    response = glue_client.get_crawler(Name=crawler_name)\n    state = response['Crawler']['State']\n    \n    print(f\"Status: {state}\")\n    \n    if state == 'READY':\n        print(\"Crawler terminou!\")\n        last_crawl = response['Crawler'].get('LastCrawl', {})\n        if last_crawl:\n            print(f\"Status: {last_crawl.get('Status')}\")\n            print(f\"Tabelas criadas/atualizadas: {last_crawl.get('TablesCreated', 0) + last_crawl.get('TablesUpdated', 0)}\")\n        break\n    \n    time.sleep(10)",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 24,
			"outputs": [
				{
					"name": "stdout",
					"text": "Status: STOPPING\nStatus: STOPPING\nStatus: STOPPING\nStatus: READY\nCrawler terminou!\nStatus: SUCCEEDED\nTabelas criadas/atualizadas: 0\n",
					"output_type": "stream"
				}
			],
			"id": "cc5efece-1d5e-42d6-94c2-5890a4d939bb"
		},
		{
			"cell_type": "code",
			"source": "response = glue_client.get_tables(DatabaseName=database_name)\n\nprint(f\"Tabelas no banco '{database_name}':\")\nfor table in response['TableList']:\n    print(f\"  {table['Name']}: {len(table['StorageDescriptor']['Columns'])} colunas\")",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 25,
			"outputs": [
				{
					"name": "stdout",
					"text": "Tabelas no banco 'clientes':\n  clientes: 21 colunas\n",
					"output_type": "stream"
				}
			],
			"id": "ce2711cd-73b0-4082-99de-a67d2542fa05"
		},
		{
			"cell_type": "code",
			"source": "athena_client = boto3.client('athena', region_name='us-east-1')\noutput_location = f\"s3://{bucket}/athena-results/\"\n\nprint(f\"Configure o Athena em: https://console.aws.amazon.com/athena\")\nprint(f\"Settings > Query result location: {output_location}\")",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 26,
			"outputs": [
				{
					"name": "stdout",
					"text": "Configure o Athena em: https://console.aws.amazon.com/athena\nSettings > Query result location: s3://pipeline-opea-bucket/athena-results/\n",
					"output_type": "stream"
				}
			],
			"id": "7929a2bd-0820-4862-9321-7c54cc8dd1cc"
		},
		{
			"cell_type": "code",
			"source": "query = f\"SELECT COUNT(*) as total FROM {database_name}.clientes\"\n\nresponse = athena_client.start_query_execution(\n    QueryString=query,\n    QueryExecutionContext={'Database': database_name},\n    ResultConfiguration={'OutputLocation': output_location}\n)\n\nquery_id = response['QueryExecutionId']\nprint(f\"Query executada! ID: {query_id}\")\n\nfor i in range(30):\n    status = athena_client.get_query_execution(QueryExecutionId=query_id)\n    state = status['QueryExecution']['Status']['State']\n    \n    if state == 'SUCCEEDED':\n        results = athena_client.get_query_results(QueryExecutionId=query_id)\n        for row in results['ResultSet']['Rows']:\n            values = [col.get('VarCharValue', 'NULL') for col in row['Data']]\n            print(f\"  {' | '.join(values)}\")\n        break\n    elif state == 'FAILED':\n        print(f\"Falha: {status['QueryExecution']['Status']['StateChangeReason']}\")\n        break\n    \n    time.sleep(2)",
			"metadata": {
				"trusted": true,
				"tags": [],
				"vscode": {
					"languageId": "python_glue_session"
				}
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "Query executada! ID: df4e2927-1026-48ba-8468-3081ee17018b\n  total\n  1\n",
					"output_type": "stream"
				}
			],
			"id": "ee0d8a4e-0f8a-497f-a743-b96dcc832f27"
		}
	]
}