Este projeto foi desenvolvido como parte de um case técnico que pedia a implementação de um pipeline de ingestão e processamento de dados. A proposta inicial sugeria a construção da solução em ambiente local ou no Databricks, com estruturação de arquivos de configuração, camadas de ingestão e transformação, além de validações e consultas analíticas. No entanto, ao longo do desenvolvimento, algumas dificuldades técnicas levaram à reavaliação do caminho e, consequentemente, à tomada de decisões arquiteturais que culminaram no uso do AWS Glue como ambiente principal de execução.

A primeira tentativa foi realizada no Databricks. A ideia inicial era manter toda a lógica dentro do notebook da plataforma, aproveitando o Spark já fornecido e a facilidade de manipulação dos dados. No entanto, como o ambiente utilizado era a versão gratuita, surgiram limitações logo no início. O cluster, por ser bloqueado, não permitia a instalação de bibliotecas adicionais necessárias para leitura de arquivos Excel, como o openpyxl. A impossibilidade de ajustar o cluster ou instalar dependências essenciais tornou inviável continuar o desenvolvimento por esse caminho.

A alternativa seguinte foi construir o pipeline em ambiente local. Foi criada uma estrutura baseada em arquivos separados de configuração, módulos, componentes de ingestão e scripts organizados, algo semelhante ao que seria feito em um ambiente profissional. Apesar disso, a execução também encontrou obstáculos. O provisionamento do ambiente local exigia dependências como Java e Hadoop completamente configurados, versões compatíveis e ajustes que, na prática, tornaram inviável até mesmo a execução mínima do pipeline. Além disso, gerar e enviar arquivos para o S3 localmente demandava configurações que não se mostraram produtivas no contexto do case, o que levou a novas interrupções no desenvolvimento.

Diante dessas limitações, a decisão final foi migrar o desenvolvimento para dentro da própria AWS, utilizando o Glue como o ambiente principal. Essa decisão foi motivada pela necessidade de simplificação, pela garantia de um ambiente gerenciado e pré-configurado, e principalmente pelo objetivo de entregar um pipeline funcional que contemplasse as camadas necessárias do case: raw, stage e analytics. A partir dessa orientação, o projeto evoluiu de forma mais fluida.

No Glue, o processo começou com a criação de um notebook PySpark. Diferentemente dos ambientes anteriores, o Glue já oferece suporte direto a PySpark, boto3 e outras dependências essenciais, eliminando grande parte dos problemas de configuração enfrentados anteriormente. O arquivo de entrada em formato xlsx foi carregado no S3 e lido diretamente pelo notebook. A partir disso, o pipeline foi estruturado para realizar a ingestão inicial na camada raw, aplicar validações como conferência de schema, detecção de duplicidade e verificação de campos obrigatórios, e, em seguida, avançar para a camada stage, onde as principais transformações ocorrem.

Com as tabelas padronizadas e validadas na camada stage, o fluxo evoluiu para a camada analytics, onde foram aplicados joins entre tabelas e formatada a estrutura final dos dados para consulta. Para disponibilizar essas informações, foi configurado um Glue Crawler — realizado tanto via código quanto com auxílio da interface — a fim de registrar a estrutura no Glue Data Catalog. Posteriormente, o Athena foi utilizado como ferramenta de consulta, permitindo que os dados processados fossem explorados por meio de SQL de forma simples e integrada ao pipeline.

Durante o desenvolvimento no Glue, algumas funcionalidades específicas, especialmente relacionadas à configuração programática de crawlers e integração com o Athena, foram implementadas com o auxílio de ferramentas de inteligência artificial. Isso permitiu superar pontos nos quais o conhecimento prévio ainda não era suficiente, como a configuração detalhada do catálogo, a criação de tabelas externas e o refinamento de consultas.

Ao final, o pipeline implementado entrega exatamente o que o case exigia: um fluxo funcional de ingestão, tratamento e disponibilização dos dados, utilizando serviços da AWS de ponta a ponta. A solução passa pelas camadas de raw, stage e analytics, aplica validações necessárias, escreve os dados de forma estruturada no S3 e ainda disponibiliza consultas no Athena. A escolha final pelo Glue trouxe simplicidade, confiabilidade e um ambiente compatível com as necessidades do projeto, além de representar uma decisão arquitetural sólida frente às dificuldades encontradas nas tentativas anteriores.

Este repositório contém o notebook utilizado no Glue, assim como o README que descreve o caminho percorrido, as decisões tomadas e a lógica aplicada ao pipeline. Ele foi estruturado de forma a demonstrar entendimento do problema, capacidade de adaptação diante de limitações técnicas e domínio do ecossistema AWS para entrega de uma solução funcional e coerente. Se fosse necessário executar novamente o pipeline, bastaria carregar o notebook no Glue, garantir as permissões adequadas, organizar as camadas no S3 e seguir a execução conforme descrito no fluxo.
